import asyncio
from datetime import datetime
from typing import List

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from fastapi import FastAPI, Query

from .config import Config
from .logger import get_logger
from .file_manager import EPGFileManager
from .epg_platform import MyTvSuper, Hami
from .epg_platform.Astro import get_astro_epg
from .epg_platform.CN_epg_pw import get_cn_channels_epg
from .epg_platform.HOY import get_hoy_epg
from .epg_platform.NowTV import request_nowtv_today_epg
from .epg_platform.RTHK import get_rthk_epg
from .epg_platform.Starhub import get_starhub_epg

logger = get_logger(__name__)

app = FastAPI(
    title=Config.APP_NAME,
    version=Config.APP_VERSION,
    description="Electronic Program Guide (EPG) aggregation service for Asian streaming platforms",
    openapi_url=None
)


@app.get("/")
async def root():
    """Health check endpoint"""
    enabled_platforms = [p["platform"] for p in Config.get_enabled_platforms()]
    return {
        "service": Config.APP_NAME,
        "version": Config.APP_VERSION,
        "status": "healthy",
        "enabled_platforms": enabled_platforms,
        "update_interval_minutes": Config.EPG_UPDATE_INTERVAL
    }


# Create scheduler instance
scheduler = AsyncIOScheduler()


@scheduler.scheduled_job('interval', minutes=Config.EPG_UPDATE_INTERVAL)
async def scheduled_epg_update():
    """Scheduled task to update EPG data from all enabled platforms"""
    logger.info(f"ğŸš€ å¼€å§‹å®šæ—¶æ›´æ–°EPGæ•°æ® - {datetime.now()}")
    await update_all_enabled_platforms()


async def request_my_tv_super_epg():
    """Update MyTV Super EPG data"""
    platform = "tvb"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        channels, programs = await MyTvSuper.get_channels(force=True)
        if not channels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{platform}çš„é¢‘é“æ•°æ®")
            return

        response_xml = await gen_channel(channels, programs)

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_hami_epg():
    """Update Hami EPG data"""
    platform = "hami"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        channels, programs = await Hami.request_all_epg()
        if not channels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{platform}çš„é¢‘é“æ•°æ®")
            return

        response_xml = await gen_channel(channels, programs)

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_cn_epg():
    """Update CN (epg.pw) EPG data"""
    platform = "cn"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        response_xml = await get_cn_channels_epg()
        if not response_xml:
            logger.warning(f"âš ï¸ æœªæ”¶åˆ°{platform}çš„EPGæ•°æ®")
            return

        # Convert string to bytes for consistent handling
        xml_bytes = response_xml.encode('utf-8') if isinstance(response_xml, str) else response_xml

        if EPGFileManager.save_epg_file(platform, xml_bytes):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_astro_epg():
    """Update Astro Go EPG data"""
    platform = "astro"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        channels, programs = await get_astro_epg()
        if not channels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{platform}çš„é¢‘é“æ•°æ®")
            return

        response_xml = await gen_channel(channels, programs)

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_rthk_epg():
    """Update RTHK EPG data"""
    platform = "rthk"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        channels, programs = await get_rthk_epg()
        if not channels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{platform}çš„é¢‘é“æ•°æ®")
            return

        response_xml = await gen_channel(channels, programs)

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_hoy_epg():
    """Update HOY EPG data"""
    platform = "hoy"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        channels, programs = await get_hoy_epg()
        if not channels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{platform}çš„é¢‘é“æ•°æ®")
            return

        response_xml = await gen_channel(channels, programs)

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_now_tv_epg():
    """Update NowTV EPG data"""
    platform = "nowtv"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        response_xml = await request_nowtv_today_epg()
        if not response_xml:
            logger.warning(f"âš ï¸ æœªæ”¶åˆ°{platform}çš„EPGæ•°æ®")
            return

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def request_starhub_epg():
    """Update StarHub EPG data"""
    platform = "starhub"
    logger.info(f"ğŸ“º æ­£åœ¨æ›´æ–°å¹³å°EPGæ•°æ®: {platform}")

    try:
        if EPGFileManager.read_epg_file(platform) is not None:
            logger.info(f"âœ… ä»Šæ—¥{platform}çš„EPGæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
            return

        channels, programs = await get_starhub_epg()
        if not channels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{platform}çš„é¢‘é“æ•°æ®")
            return

        response_xml = await gen_channel(channels, programs)

        if EPGFileManager.save_epg_file(platform, response_xml):
            EPGFileManager.delete_old_epg_files(platform)
            logger.info(f"âœ¨ æˆåŠŸæ›´æ–°{platform}çš„EPGæ•°æ®")
        else:
            logger.error(f"âŒ ä¿å­˜{platform}çš„EPGæ–‡ä»¶å¤±è´¥")

    except Exception as e:
        logger.error(f"ğŸ’¥ æ›´æ–°{platform}çš„EPGæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


@app.get("/epg/{platform}")
async def get_platform_epg(platform: str):
    """Get EPG data for a specific platform"""
    logger.info(f"ğŸ“¡ æä¾›å¹³å°EPGæ•°æ®æœåŠ¡: {platform}")
    return EPGFileManager.get_single_platform_epg(platform)


@app.get("/epg")
async def get_custom_aggregate_epg(platforms: str = Query(..., description="Comma-separated platform list in priority order")):
    """
    Get aggregated EPG data from custom platform selection

    Example: ?platforms=tvb,nowtv,hami
    """
    platform_list = [p.strip() for p in platforms.split(',') if p.strip()]
    logger.info(f"ğŸ“Š æä¾›è‡ªå®šä¹‰èšåˆEPGæ•°æ®æœåŠ¡: {platform_list}")
    return EPGFileManager.aggregate_epg_files(platform_list)


@app.get("/all")
async def get_all_enabled_platforms_epg():
    """Get aggregated EPG data from all enabled platforms (cached)"""
    logger.info(f"ğŸŒ æä¾›allå¹³å°çš„ç¼“å­˜EPGæ•°æ®æœåŠ¡")
    return EPGFileManager.get_single_platform_epg("all")


@app.get("/all.gz")
async def get_all_enabled_platforms_epg_gz():
    """Get aggregated EPG data from all enabled platforms (cached, gzip compressed)"""
    from fastapi.responses import FileResponse
    import os

    logger.info(f"ğŸ“¦ æä¾›allå¹³å°çš„gzå‹ç¼©ç¼“å­˜EPGæ•°æ®æœåŠ¡")

    gz_file_path = EPGFileManager.get_epg_file_path("all").replace(".xml", ".xml.gz")

    if not os.path.exists(gz_file_path):
        logger.error(f"âŒ æœªæ‰¾åˆ°all.gzå‹ç¼©æ–‡ä»¶: {gz_file_path}")
        from fastapi import HTTPException
        raise HTTPException(
            status_code=404,
            detail="Compressed EPG data not available. Please wait for next update cycle."
        )

    return FileResponse(
        path=gz_file_path,
        media_type="application/gzip",
        headers={
            "Content-Disposition": "attachment; filename=epg.xml.gz",
            "Cache-Control": f"public, max-age={Config.EPG_CACHE_TTL}, s-maxage={Config.EPG_CACHE_TTL}",
            "ETag": f'"epg-all-gz-{datetime.now().strftime("%Y%m%d")}"'
        },
        filename="epg.xml.gz"
    )


async def gen_channel(channels, programs):
    """Generate EPG XML from channels and programs data"""
    from .epg.EpgGenerator import generateEpg
    return await generateEpg(channels, programs)


async def generate_all_platforms_cache():
    """Generate and cache merged EPG for all enabled platforms"""
    enabled_platforms = [p["platform"] for p in Config.get_enabled_platforms()]

    if not enabled_platforms:
        logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨ä»»ä½•å¹³å°ï¼Œæ— æ³•ç”Ÿæˆallç¼“å­˜")
        return

    logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆallå¹³å°åˆå¹¶ç¼“å­˜: {enabled_platforms}")

    try:
        # Use existing aggregate logic to merge all platforms
        import xml.etree.ElementTree as ET
        import gzip

        merged_root = ET.Element("tv")
        merged_root.set("generator-info-name", f"{Config.APP_NAME} v{Config.APP_VERSION}")
        merged_root.set("generator-info-url", "https://github.com/your-repo/CharmingEPG")

        channels_seen = set()
        total_channels = 0
        total_programs = 0

        for platform in enabled_platforms:
            content = EPGFileManager.read_epg_file(platform)
            if not content:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å¹³å°çš„EPGæ•°æ®: {platform}")
                continue

            try:
                platform_root = ET.fromstring(content)

                # Process channels (first-come-first-served for duplicates)
                platform_channels = 0
                platform_programs = 0

                for channel in platform_root.findall("./channel"):
                    channel_id = channel.get("id")
                    if channel_id and channel_id not in channels_seen:
                        channels_seen.add(channel_id)
                        merged_root.append(channel)
                        platform_channels += 1

                        # Add all programs for this channel
                        for programme in platform_root.findall(f"./programme[@channel='{channel_id}']"):
                            merged_root.append(programme)
                            platform_programs += 1

                total_channels += platform_channels
                total_programs += platform_programs

                logger.debug(f"ğŸ”€ ä»{platform}åˆå¹¶{platform_channels}ä¸ªé¢‘é“å’Œ{platform_programs}ä¸ªèŠ‚ç›®")

            except ET.ParseError as e:
                logger.error(f"âŒ è§£æå¹³å°{platform}çš„XMLå¤±è´¥: {e}")
                continue

        if total_channels == 0:
            logger.error("âŒ ä»»ä½•å¹³å°éƒ½æœªæ‰¾åˆ°æœ‰æ•ˆçš„EPGæ•°æ®ï¼Œæ— æ³•ç”Ÿæˆallç¼“å­˜")
            return

        # Convert merged XML to bytes
        merged_xml = ET.tostring(merged_root, encoding="utf-8", xml_declaration=True)

        # Save to cache file using "all" as platform name
        if EPGFileManager.save_epg_file("all", merged_xml):
            logger.info(f"âœ¨ æˆåŠŸç”Ÿæˆallç¼“å­˜: {total_channels}ä¸ªé¢‘é“å’Œ{total_programs}ä¸ªèŠ‚ç›®")
        else:
            logger.error("âŒ ä¿å­˜allç¼“å­˜æ–‡ä»¶å¤±è´¥")

        # Generate gzip compressed version
        compressed_xml = gzip.compress(merged_xml, compresslevel=9)
        gz_file_path = EPGFileManager.get_epg_file_path("all").replace(".xml", ".xml.gz")

        try:
            EPGFileManager.ensure_directory_exists(gz_file_path)
            with open(gz_file_path, "wb") as gz_file:
                gz_file.write(compressed_xml)

            compression_ratio = len(compressed_xml) / len(merged_xml) * 100
            saved_ratio = 100 - compression_ratio
            logger.info(f"ğŸ“¦ æˆåŠŸç”Ÿæˆall.gzå‹ç¼©ç¼“å­˜: {len(compressed_xml)} å­—èŠ‚ (å‹ç¼©è‡³åŸæ¥çš„ {compression_ratio:.1f}%ï¼ŒèŠ‚çœ {saved_ratio:.1f}%)")
        except Exception as gz_error:
            logger.error(f"âŒ ä¿å­˜all.gzå‹ç¼©æ–‡ä»¶å¤±è´¥: {gz_error}")

    except Exception as e:
        logger.error(f"ğŸ’¥ ç”Ÿæˆallç¼“å­˜æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def update_all_enabled_platforms():
    """Update EPG data for all enabled platforms"""
    enabled_platforms = Config.get_enabled_platforms()

    if not enabled_platforms:
        logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨ä»»ä½•å¹³å°")
        return

    logger.info(f"ğŸ”„ å¼€å§‹æ›´æ–°{len(enabled_platforms)}ä¸ªå¯ç”¨å¹³å°çš„EPGæ•°æ®")

    tasks = [
        globals()[conf["fetcher"]]()
        for conf in enabled_platforms
    ]

    # Execute all tasks in parallel
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results and log any exceptions
    success_count = 0
    error_count = 0
    any_platform_updated = False

    for i, result in enumerate(results):
        platform_config = enabled_platforms[i]
        platform_name = platform_config["name"]

        if isinstance(result, Exception):
            error_count += 1
            logger.error(f"âŒ æ›´æ–°{platform_name}çš„EPGæ•°æ®å¤±è´¥: {result}", exc_info=True)
        else:
            success_count += 1
            logger.debug(f"âœ… æˆåŠŸæ›´æ–°{platform_name}çš„EPGæ•°æ®")

    logger.info(f"ğŸ¯ EPGæ•°æ®æ›´æ–°å®Œæˆ: {success_count}ä¸ªæˆåŠŸï¼Œ{error_count}ä¸ªå¤±è´¥")

    # Check if all cache exists, if not, we need to generate it
    all_cache_exists = EPGFileManager.read_epg_file("all") is not None

    # Generate merged cache for /all endpoint if:
    # 1. Cache doesn't exist (first run or new day)
    # 2. At least one platform was updated successfully
    if not all_cache_exists:
        logger.info("ğŸ“ allç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹ç”Ÿæˆ")
        await generate_all_platforms_cache()
    else:
        logger.info("âœ… allç¼“å­˜å·²å­˜åœ¨ä¸”æ‰€æœ‰å¹³å°å‡æœªæ›´æ–°ï¼Œè·³è¿‡é‡æ–°ç”Ÿæˆ")


@app.on_event("startup")
async def startup():
    """Application startup event"""
    logger.info(f"ğŸš€ å¯åŠ¨ {Config.APP_NAME} v{Config.APP_VERSION}")
    logger.info(f"â° EPGæ›´æ–°é—´éš”: {Config.EPG_UPDATE_INTERVAL} åˆ†é’Ÿ")

    enabled_platforms = [p["name"] for p in Config.get_enabled_platforms()]
    logger.info(f"ğŸ“º å·²å¯ç”¨å¹³å°: {', '.join(enabled_platforms)}")

    # Start the scheduler
    scheduler.start()
    logger.info("âš¡ è°ƒåº¦å™¨å·²å¯åŠ¨")

    # Trigger initial EPG update
    asyncio.create_task(update_all_enabled_platforms())
    logger.info("ğŸ¬ åˆå§‹EPGæ•°æ®æ›´æ–°å·²è§¦å‘")